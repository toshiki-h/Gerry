{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import tqdm\n",
    "import time\n",
    "import pymongo\n",
    "log = logging.getLogger('gerry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_logging(data_dir):\n",
    "    global log\n",
    "    log.setLevel(logging.DEBUG)\n",
    "    log_name = os.path.join(data_dir, 'gerry-crawl.log')\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)-8s %(message)s')\n",
    "    file_handler = logging.FileHandler(log_name)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    log.addHandler(file_handler)\n",
    "    return log\n",
    "\n",
    "\n",
    "def create_time_frames(from_datetime, to_datetime, frame_size):\n",
    "    # [from_datetime, to_datetime[\n",
    "    result = []\n",
    "    time_frame_start = from_datetime\n",
    "    time_frame_end = from_datetime + frame_size + \\\n",
    "        datetime.timedelta(milliseconds=-1)\n",
    "    while time_frame_end <= to_datetime:\n",
    "        result += [(time_frame_start, time_frame_end)]\n",
    "        time_frame_start += frame_size\n",
    "        time_frame_end += frame_size\n",
    "    return result\n",
    "\n",
    "\n",
    "def datetime_to_string(date):\n",
    "    return date.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]\n",
    "\n",
    "class Gerry(object):\n",
    "    def __init__(self, name, url, start_date, end_date,\n",
    "                 directory='./gerry_data/'):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.directory = os.path.join(directory, name)\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        os.makedirs(self.directory, exist_ok=True)\n",
    "\n",
    "    def wait_for_server(status_code):\n",
    "        # https://cloud.google.com/service-control/troubleshooting#how_do_i_perform_a_retry_on_api_errors\n",
    "        GOOGLE_SERVER_WAITING_TIME = {429: 31, 500: 1, 503: 1}\n",
    "        if status_code in GOOGLE_SERVER_WAITING_TIME:\n",
    "            time.sleep(GOOGLE_SERVER_WAITING_TIME[status_code])\n",
    "\n",
    "    def handle_exception(exception, change_type):\n",
    "        if isinstance(exception, requests.exceptions.RequestException):\n",
    "            if exception.response is not None:\n",
    "                log.error('GET %s failed with http status %i' % (\n",
    "                    change_type, exception.response.status_code))\n",
    "                Gerry.wait_for_server(\n",
    "                    exception.response.status_code)\n",
    "            else:\n",
    "                log.error('GET %s %s failed with error: %s' % (change_type,\n",
    "                                                               exception))\n",
    "        elif isinstance(exception, json.JSONDecodeError):\n",
    "            log.error(\n",
    "                'Reading JSON for %s failed' % (change_type))\n",
    "        elif isinstance(exception, Exception):\n",
    "            log.error('Unknown error occurred for %s %s: %s' % (change_type,\n",
    "                                                                exception))\n",
    "\n",
    "    def get_changes(self, day):\n",
    "        from_datetime = day\n",
    "        to_datetime = from_datetime + \\\n",
    "            datetime.timedelta(hours=24) + datetime.timedelta(milliseconds=-1)\n",
    "        more_changes = True\n",
    "        changes = []\n",
    "        offset = 0\n",
    "\n",
    "        while more_changes:\n",
    "            changes_subset = []\n",
    "            url = '%s/changes/?q=after:{%s} AND before:{%s}&o=ALL_COMMITS&o=ALL_REVISIONS&S=%i' % (\n",
    "                self.url, datetime_to_string(from_datetime), datetime_to_string(to_datetime), offset)\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            changes_subset = json.loads(response.text[5:])\n",
    "            if changes_subset:\n",
    "                more_changes = '_more_changes' in changes_subset[-1]\n",
    "                changes += changes_subset\n",
    "            else:\n",
    "                more_changes = False\n",
    "            offset += len(changes_subset)\n",
    "        return changes\n",
    "\n",
    "    def get_change(self, change_number, folder):\n",
    "        # Extract comments\n",
    "        comments_inlines = {'comments': [], 'inlines': []}\n",
    "        url = '%s/changes/%s/detail/?o=DETAILED_LABELS&o=MESSAGES&o=DETAILED_ACCOUNTS&o=REVIEWED&o=ALL_FILES&o=ALL_COMMITS&o=ALL_REVISIONS' % (\n",
    "            self.url, change_number)\n",
    "        if self.name != 'libreoffice':\n",
    "            url += '&o=REVIEWER_UPDATES'\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        details = json.loads(response.text[5:])\n",
    "#         comments = {'_number': details['_number'], 'change_id': details['change_id'], 'project': details['project'], \n",
    "#                    'created': details['created'], 'updated': details['updated'], 'owner': details['owner'],\n",
    "#                    'revisionNums': details['revisions'][details['current_revision']]['_number'],\n",
    "#                    'insertions': details['insertions'], 'deletions': details['deletions']}\n",
    "        comments = {'_number': details['_number'], 'change_id': details['change_id'], 'project': details['project'], \n",
    "                   'created': details['created'], 'updated': details['updated'], 'owner': details['owner'],\n",
    "                   'revisionNums': details['revisions'][details['current_revision']]['_number'],\n",
    "                   'insertions': details['insertions'], 'deletions': details['deletions'], 'messages': details['messages']}\n",
    "        comments_inlines['comments'] = comments\n",
    "        #print({'_number': details['_number'], 'updated': details['updated'], 'revisionNums': details['revisions'][details['current_revision']]['_number']})\n",
    "        # Extract inlines\n",
    "        revision_numbers = int(details['revisions'][details['current_revision']]['_number'])\n",
    "        inlines = []\n",
    "        for revision_number in range(1, revision_numbers+1):\n",
    "            url = '%s/changes/%s/revisions/%s/comments' % (\n",
    "                self.url, change_number, revision_number)\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            inline = json.loads(response.text[5:])\n",
    "            inline_replaced = {}\n",
    "            for fileKey in inline.keys():\n",
    "                inline_replaced[fileKey.replace('.', '_')] = inline[fileKey]\n",
    "            inline_replaced['_number'] = details['_number']\n",
    "            inline_replaced['change_id'] = details['change_id']\n",
    "            inline_replaced['project'] = details['project']\n",
    "            #print(inline_replaced)\n",
    "            inlines.append(inline_replaced)\n",
    "        comments_inlines['inlines'] = inlines\n",
    "        return comments_inlines\n",
    "        \n",
    "        # Dump data\n",
    "#         file_name = str(change_number) + '.json'\n",
    "#         with open(os.path.join(folder, file_name), 'w') as json_file:\n",
    "#             json.dump(change, json_file)\n",
    "\n",
    "    def run(self, changes_collection, comments_collection, inlines_collection):\n",
    "        all_day_paths = []\n",
    "        for time_frame in create_time_frames(\n",
    "                self.start_date, self.end_date, datetime.timedelta(hours=24)):\n",
    "            day_str = time_frame[0].strftime('%Y-%m-%d')\n",
    "            #os.makedirs(os.path.join(self.directory, 'changes', day_str), exist_ok=True)\n",
    "            all_day_paths.append(day_str)\n",
    "\n",
    "        #all_day_paths = glob.glob(os.path.join(self.directory, 'changes', '*'))\n",
    "        print(all_day_paths)\n",
    "        complete = False\n",
    "\n",
    "        #while not complete:\n",
    "        complete = True\n",
    "#             day_paths_pending = [\n",
    "#                 day_path for day_path in all_day_paths if not os.listdir(day_path)]\n",
    "        day_paths_pending = [\n",
    "            day_path for day_path in all_day_paths]            \n",
    "\n",
    "        log.info(\n",
    "            'Started new crawl iteration to crawl %i pending days' % (len(day_paths_pending)))\n",
    "\n",
    "        for day_path in tqdm.tqdm(day_paths_pending):\n",
    "            change_numbers = []\n",
    "            changes = []\n",
    "            log.info(\n",
    "                'Crawling review data on %s ' % (day_path))\n",
    "            day = datetime.datetime.strptime(\n",
    "                day_path, '%Y-%m-%d')\n",
    "            try:\n",
    "                # Extract Basic Review Info\n",
    "                changes = self.get_changes(day)\n",
    "                if len(changes) > 0:\n",
    "                    changes_collection.insert_many(changes)\n",
    "                else:\n",
    "                    logging.info('There is no change on %s'  % (day))\n",
    "            except Exception as exception:\n",
    "                Gerry.handle_exception(exception, 'changes on ' + str(day))\n",
    "                complete = False\n",
    "\n",
    "            change_numbers += [change['_number'] for change in changes]\n",
    "\n",
    "            for change_number in change_numbers:\n",
    "                try:\n",
    "                    comments_inline = self.get_change(change_number, day_path)\n",
    "                    #print(type(comments_inline['comments']))\n",
    "                    if len(comments_inline['comments']) > 0:\n",
    "                        comments_collection.insert_many([comments_inline['comments']])\n",
    "                    if len(comments_inline['inlines']) > 0:\n",
    "                        #print(type(comments_inline['inlines']))\n",
    "                        inlines_collection.insert_many(comments_inline['inlines'])\n",
    "                except Exception as exception:\n",
    "                    Gerry.handle_exception(\n",
    "                        exception, 'change ' + str(change_number))\n",
    "                    complete = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    db_name = 'android'\n",
    "    project_url = 'https://android-review.googlesource.com'\n",
    "    client = pymongo.MongoClient()\n",
    "    db = client[db_name]\n",
    "    changes_collection = db['reviews']\n",
    "    comments_collection = db['comments']\n",
    "    inlines_collection = db['inlines']\n",
    "    \n",
    "    gerry = Gerry(db_name, project_url, \n",
    "                  datetime.datetime(2008, 7, 1), datetime.datetime(2018, 8, 1), directory='./gerry_data/') \n",
    "    config_logging(gerry.directory)\n",
    "    \n",
    "    gerry.run(changes_collection, comments_collection, inlines_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project",
   "language": "python",
   "name": "my_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
